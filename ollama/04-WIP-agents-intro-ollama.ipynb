{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Essentials Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain's Agents, adding the ability to use tools such as search and calculators to complete tasks that normal LLMs cannot fufil. In this example we will be using Meta's `llama3.2:1b-instruct-fp16`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ⚠️ We will be using Meta for this example allowing us to run everything via API. If you would like to use OpenAI instead, please see the [OpenAI version](https://github.com/aurelio-labs/langchain-course/blob/main/notebooks/openai/04-agents-intro-openai.ipynb) of this example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools are a way augment our LLMs with code execution. A tool is simply a function formatted so that our agent can undertstand how to use it, and then execute it. Let's start by creating a few simple tools.\n",
    "\n",
    "We can use the `@tool` decorator to create an LLM-compatible tool from a standard python function — this function should include a few things for optimal performance:\n",
    "\n",
    "* A docstring describing what the tool does and when it should be used, this will be read by our LLM/agent and used to decide when to use the tool, and also how to use the tool.\n",
    "\n",
    "* Clear parameter names that ideally tell the LLM what each parameter is, if it isn't clear we make sure the docstring explains what the parameter is for and how to use it.\n",
    "\n",
    "* Both parameter and return type annotations."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:37:41.437845Z",
     "start_time": "2025-06-12T15:37:41.434126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:29:52.788407Z",
     "start_time": "2025-06-12T15:29:52.169179Z"
    }
   },
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'. Both 'x' and 'y' are numbers.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'. Both 'x' and 'y' are numbers.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'. Both 'x' and 'y' are numbers.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'. Both 'x' and 'y' are numbers.\"\"\"\n",
    "    return y - x"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `@tool` decorator our function is turned into a `StructuredTool` object, which we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:29:56.535862Z",
     "start_time": "2025-06-12T15:29:56.530657Z"
    }
   },
   "source": [
    "add"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='add', description=\"Add 'x' and 'y'. Both 'x' and 'y' are numbers.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x0000017054C65120>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the tool name, description, and arg schema:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:29:57.242438Z",
     "start_time": "2025-06-12T15:29:57.239035Z"
    }
   },
   "source": [
    "print(f\"{add.name=}\\n{add.description=}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add.name='add'\n",
      "add.description=\"Add 'x' and 'y'. Both 'x' and 'y' are numbers.\"\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:29:58.507629Z",
     "start_time": "2025-06-12T15:29:58.503494Z"
    }
   },
   "source": [
    "add.args_schema.model_json_schema()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Add 'x' and 'y'. Both 'x' and 'y' are numbers.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'add',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:29:59.006502Z",
     "start_time": "2025-06-12T15:29:59.002206Z"
    }
   },
   "source": [
    "exponentiate.args_schema.model_json_schema()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Raise 'x' to the power of 'y'. Both 'x' and 'y' are numbers.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'exponentiate',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:29:59.918065Z",
     "start_time": "2025-06-12T15:29:59.915053Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "llm_output_string = \"{\\\"x\\\": 5, \\\"y\\\": 2}\"  # this is the output from the LLM\n",
    "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
    "llm_output_dict"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 5, 'y': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then passed into the tool function as `kwargs` (keyword arguments) as indicated by the `**` operator - the `**` operator is used to unpack the dictionary into keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:30:03.821244Z",
     "start_time": "2025-06-12T15:30:03.817134Z"
    }
   },
   "source": [
    "exponentiate.func(**llm_output_dict)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers the basics of tools and how they work, let's move on to creating the agent itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to construct a simple tool calling agent. We will use **L**ang**C**hain **E**pression **L**anguage (LCEL) to construct the agent. We will cover LCEL more in the next chapter, but for now - all we need to know is that our agent will be constructed using syntax and components like so:\n",
    "\n",
    "\n",
    "```\n",
    "agent = (\n",
    "    <input parameters, including chat history and user query>\n",
    "    | <prompt>\n",
    "    | <LLM with tools>\n",
    ")\n",
    "```\n",
    "\n",
    "We need this agent to remember previous interactions within the conversation. To do that, we will use the `ChatPromptTemplate` with a system message, a placeholder for our chat history, a placeholder for the user query, and finally a placeholder for the agent scratchpad.\n",
    "\n",
    "The agent scratchpad is where the agent will write it's _\"notes\"_ as it is working through multiple internal thought and tool-use steps to produce a final output to the user."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:30:08.436008Z",
     "start_time": "2025-06-12T15:30:08.380671Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You're a helpful assistant. Using the tools provided you must answer the \"\n",
    "        \"user's questions. To use the tool you must always provide the correct JSON \"\n",
    "        \"format for the tool input. For example, if adding two numbers you would use \"\n",
    "        \"the `add` tool, passing the two numbers to the `x` and `y` parameters.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing the 32B parameter QwQ. QwQ performs very well on reasoning, maths, and tool use. We pull the model from Ollama by switching to our terminal and executing:\n",
    "\n",
    "```\n",
    "ollama pull qwq:32b\n",
    "```\n",
    "\n",
    "Once the model has finished downloading, we initialize it in LangChain using the ChatOllama class:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:30:15.351263Z",
     "start_time": "2025-06-12T15:30:14.709270Z"
    }
   },
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_name = \"qwen3:8b\"\n",
    "\n",
    "# initialize one LLM with temperature 0.0, this makes the LLM more deterministic\n",
    "llm = ChatOllama(temperature=0.0, model=model_name)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating an agent we need to add conversational memory to make the agent remember previous interactions. We'll be using the older `ConversationBufferMemory` class rather than the newer `RunnableWithMessageHistory` — the reason being that we will also be using the older `create_tool_calling_agent` and `AgentExecutor` method and class.\n",
    "\n",
    "In the `05` chapter we will be using the newer `RunnableWithMessageHistory` class as we'll be building a custom `AgentExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:30:19.579803Z",
     "start_time": "2025-06-12T15:30:19.506207Z"
    }
   },
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # must align with MessagesPlaceholder variable_name\n",
    "    return_messages=True  # to return Message objects\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toni\\AppData\\Local\\Temp\\ipykernel_1844\\4092651347.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will initialize our agent. For that we need:\n",
    "\n",
    "* `llm`: as already defined\n",
    "* `tools`: to be defined (just a list of our previously defined tools)\n",
    "* `prompt`: as already defined\n",
    "* `memory`: as already defined"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:30:24.172910Z",
     "start_time": "2025-06-12T15:30:23.672952Z"
    }
   },
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate]\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=llm, tools=tools, prompt=prompt\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `agent` by itself is like one-step of our agent execution loop. So, if we call the `agent.invoke` method it will get the LLM to generate a single response and go no further, so no tools will be executed, and no next iterations will be performed.\n",
    "\n",
    "We can see this by asking a query that should trigger a tool call:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:30:59.700139Z",
     "start_time": "2025-06-12T15:30:26.972306Z"
    }
   },
   "source": [
    "agent.invoke({\n",
    "    \"input\": \"what is 10 multiplied by 7?\",\n",
    "    \"chat_history\": memory.chat_memory.messages,\n",
    "    \"intermediate_steps\": []  # agent will append it's internal steps here\n",
    "})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolAgentAction(tool='multiply', tool_input={'x': 10, 'y': 7}, log='\\nInvoking: `multiply` with `{\\'x\\': 10, \\'y\\': 7}`\\nresponded: <think>\\nOkay, the user is asking, \"what is 10 multiplied by 7?\" So I need to figure out which tool to use here. Let me check the available functions. There\\'s add, subtract, multiply, and exponentiate. The question is about multiplication, so the multiply function is the right choice. The parameters required are x and y, both numbers. The user provided 10 and 7. So I should call the multiply function with x=10 and y=7. Let me make sure I\\'m not mixing up the order. The function\\'s description says multiply x and y, so 10 multiplied by 7. Yep, that\\'s correct. I\\'ll format the tool call accordingly.\\n</think>\\n\\n\\n\\n', message_log=[AIMessage(content='<think>\\nOkay, the user is asking, \"what is 10 multiplied by 7?\" So I need to figure out which tool to use here. Let me check the available functions. There\\'s add, subtract, multiply, and exponentiate. The question is about multiplication, so the multiply function is the right choice. The parameters required are x and y, both numbers. The user provided 10 and 7. So I should call the multiply function with x=10 and y=7. Let me make sure I\\'m not mixing up the order. The function\\'s description says multiply x and y, so 10 multiplied by 7. Yep, that\\'s correct. I\\'ll format the tool call accordingly.\\n</think>\\n\\n', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-06-12T15:30:59.6944894Z', 'done': True, 'done_reason': 'stop', 'total_duration': 32537342700, 'load_duration': 4352249100, 'prompt_eval_count': 448, 'prompt_eval_duration': 12025471200, 'eval_count': 177, 'eval_duration': 16150059300, 'model_name': 'qwen3:8b'}, id='run--058d7cdc-7def-4a35-8fd3-7a780b3174bc-0', tool_calls=[{'name': 'multiply', 'args': {'x': 10, 'y': 7}, 'id': 'e17439db-d014-4ea2-bbe2-684ec5881163', 'type': 'tool_call'}], usage_metadata={'input_tokens': 448, 'output_tokens': 177, 'total_tokens': 625})], tool_call_id='e17439db-d014-4ea2-bbe2-684ec5881163')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the LLM has generated that we should use the `multiply` tool and the tool input should be `{\"x\": 10.7, \"y\": 7.68}`. However, the tool is not executed. For that to happen we need an agent execution loop, which will handle the multiple iterations of generation to tool calling to generation, etc.\n",
    "\n",
    "We use the `AgentExecutor` class to handle the execution loop:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:31:48.352107Z",
     "start_time": "2025-06-12T15:31:48.349129Z"
    }
   },
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the same query with the executor, note that the `intermediate_steps` parameter that we added before is no longer needed as the executor handles it internally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:32:10.129935Z",
     "start_time": "2025-06-12T15:31:51.237171Z"
    }
   },
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"what is 10 multiplied by 7?\",\n",
    "    \"chat_history\": memory.chat_memory.messages,\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `multiply` with `{'x': 10, 'y': 7}`\n",
      "responded: <think>\n",
      "Okay, the user is asking, \"what is 10 multiplied by 7?\" So I need to figure out which tool to use here. Let me check the available functions. There's add, subtract, multiply, and exponentiate. The question is about multiplication, so the multiply function is the right choice. The parameters required are x and y, both numbers. The user provided 10 and 7. So I should call the multiply function with x=10 and y=7. Let me make sure I'm not mixing up the order. The function's description says multiply x and y, so 10 multiplied by 7. Yep, that's correct. I'll format the tool call accordingly.\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[38;5;200m\u001B[1;3m70.0\u001B[0m\u001B[32;1m\u001B[1;3m<think>\n",
      "</think>\n",
      "\n",
      "The result of multiplying 10 by 7 is **70**.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 10 multiplied by 7?',\n",
       " 'chat_history': [HumanMessage(content='what is 10 multiplied by 7?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='<think>\\n</think>\\n\\nThe result of multiplying 10 by 7 is **70**.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': '<think>\\n</think>\\n\\nThe result of multiplying 10 by 7 is **70**.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `multiply` tool was invoked, producing the observation of `70.0`. After the observation was provided, we can see that the LLM then generated a final response of:\n",
    "\n",
    "```\n",
    "The result of multiplying 10 by 7 is 70.\n",
    "```\n",
    "\n",
    "This final response was generated based on the original query and the tool output (ie the _observation_). We can also confirm that this answer is accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our agent with some memory and tool use. First, we tell it our name, then we will perform a few tool calls, then see if the agent can still recall our name.\n",
    "\n",
    "First, give the agent our name:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:32:33.544600Z",
     "start_time": "2025-06-12T15:32:14.122113Z"
    }
   },
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"My name is Josh\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m<think>\n",
      "Okay, the user said, \"My name is Josh.\" Let me think about how to respond.\n",
      "\n",
      "First, I need to acknowledge their name. Since they introduced themselves, a polite response would be to greet them by name. The previous interaction was about multiplying 10 by 7, and they provided the correct answer. Now, they're sharing their name, so I should make sure to welcome them properly.\n",
      "\n",
      "I should check if there's any specific function I need to use here. Looking at the tools provided, the functions are add, subtract, multiply, and exponentiate. Since the user is just stating their name, there's no mathematical operation needed here. The response should be a simple greeting without any tool calls.\n",
      "\n",
      "So, the appropriate response would be something like, \"Hello, Josh! Welcome.\" That's friendly and acknowledges their introduction. No need to use any of the functions in this case.\n",
      "</think>\n",
      "\n",
      "Hello, Josh! Welcome. How can I assist you today?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My name is Josh',\n",
       " 'chat_history': [HumanMessage(content='what is 10 multiplied by 7?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='<think>\\n</think>\\n\\nThe result of multiplying 10 by 7 is **70**.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Josh', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='<think>\\nOkay, the user said, \"My name is Josh.\" Let me think about how to respond.\\n\\nFirst, I need to acknowledge their name. Since they introduced themselves, a polite response would be to greet them by name. The previous interaction was about multiplying 10 by 7, and they provided the correct answer. Now, they\\'re sharing their name, so I should make sure to welcome them properly.\\n\\nI should check if there\\'s any specific function I need to use here. Looking at the tools provided, the functions are add, subtract, multiply, and exponentiate. Since the user is just stating their name, there\\'s no mathematical operation needed here. The response should be a simple greeting without any tool calls.\\n\\nSo, the appropriate response would be something like, \"Hello, Josh! Welcome.\" That\\'s friendly and acknowledges their introduction. No need to use any of the functions in this case.\\n</think>\\n\\nHello, Josh! Welcome. How can I assist you today?', additional_kwargs={}, response_metadata={})],\n",
       " 'output': '<think>\\nOkay, the user said, \"My name is Josh.\" Let me think about how to respond.\\n\\nFirst, I need to acknowledge their name. Since they introduced themselves, a polite response would be to greet them by name. The previous interaction was about multiplying 10 by 7, and they provided the correct answer. Now, they\\'re sharing their name, so I should make sure to welcome them properly.\\n\\nI should check if there\\'s any specific function I need to use here. Looking at the tools provided, the functions are add, subtract, multiply, and exponentiate. Since the user is just stating their name, there\\'s no mathematical operation needed here. The response should be a simple greeting without any tool calls.\\n\\nSo, the appropriate response would be something like, \"Hello, Josh! Welcome.\" That\\'s friendly and acknowledges their introduction. No need to use any of the functions in this case.\\n</think>\\n\\nHello, Josh! Welcome. How can I assist you today?'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and get the agent to perform multiple tool calls within a single execution loop:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-12T14:57:02.130740Z"
    }
   },
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"What is nine plus 10, minus 4 * 2, to the power of 3\", \n",
    "    \"chat_history\": memory\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the answer is accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9+10-(4*2)**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now let's see if the agent can still recall our name:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:32:58.940805Z",
     "start_time": "2025-06-12T15:32:48.129771Z"
    }
   },
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"What is my name\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m<think>\n",
      "Okay, the user asked, \"What is my name?\" Let me check the conversation history.\n",
      "\n",
      "Earlier, the user said, \"My name is Josh,\" and I responded by asking how I could assist. Now they're asking for their name. Since I have access to the previous messages, I should recall that their name is Josh. I don't need to use any tools here because the information is already in the chat history. Just need to provide the answer directly without calling a function.\n",
      "</think>\n",
      "\n",
      "Your name is Josh.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name',\n",
       " 'chat_history': [HumanMessage(content='what is 10 multiplied by 7?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='<think>\\n</think>\\n\\nThe result of multiplying 10 by 7 is **70**.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Josh', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='<think>\\nOkay, the user said, \"My name is Josh.\" Let me think about how to respond.\\n\\nFirst, I need to acknowledge their name. Since they introduced themselves, a polite response would be to greet them by name. The previous interaction was about multiplying 10 by 7, and they provided the correct answer. Now, they\\'re sharing their name, so I should make sure to welcome them properly.\\n\\nI should check if there\\'s any specific function I need to use here. Looking at the tools provided, the functions are add, subtract, multiply, and exponentiate. Since the user is just stating their name, there\\'s no mathematical operation needed here. The response should be a simple greeting without any tool calls.\\n\\nSo, the appropriate response would be something like, \"Hello, Josh! Welcome.\" That\\'s friendly and acknowledges their introduction. No need to use any of the functions in this case.\\n</think>\\n\\nHello, Josh! Welcome. How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='<think>\\nOkay, the user asked, \"What is my name?\" Let me check the conversation history.\\n\\nEarlier, the user said, \"My name is Josh,\" and I responded by asking how I could assist. Now they\\'re asking for their name. Since I have access to the previous messages, I should recall that their name is Josh. I don\\'t need to use any tools here because the information is already in the chat history. Just need to provide the answer directly without calling a function.\\n</think>\\n\\nYour name is Josh.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': '<think>\\nOkay, the user asked, \"What is my name?\" Let me check the conversation history.\\n\\nEarlier, the user said, \"My name is Josh,\" and I responded by asking how I could assist. Now they\\'re asking for their name. Since I have access to the previous messages, I should recall that their name is Josh. I don\\'t need to use any tools here because the information is already in the chat history. Just need to provide the answer directly without calling a function.\\n</think>\\n\\nYour name is Josh.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent has successfully recalled our name. Let's move on to another agent example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SerpAPI Weather Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll be using the same agent and executor setup as before, but we'll be adding the [SerpAPI](https://serpapi.com/users/sign_in) service to allow our agent to search the web for information.\n",
    "\n",
    "To use this tool, you need an API key, with the free plan you can use up to 100 searches per month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will load the `serpapi` tool directly from the prebuilt tools that LangChain provides."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:40:12.661868Z",
     "start_time": "2025-06-12T15:40:12.658252Z"
    }
   },
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "search = TavilySearchResults()\n",
    "toolbox = [search]\n",
    "# toolbox = load_tools(tool_names=['serpapi'], llm=llm)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These custom tools can look into your IP address, find out where you are currently, then we will also use a secondary function to get the current date and time, then we will use this information to feed into the SerpAPI to find us the weather pattern in your area and at the time of the function calling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:40:14.045055Z",
     "start_time": "2025-06-12T15:40:14.039191Z"
    }
   },
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "@tool\n",
    "def get_location_from_ip():\n",
    "    \"\"\"Get the geographical location based on the IP address.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"https://ipinfo.io/json\")\n",
    "        data = response.json()\n",
    "        if 'loc' in data:\n",
    "            latitude, longitude = data['loc'].split(',')\n",
    "            data = (\n",
    "                f\"Latitude: {latitude},\\n\"\n",
    "                f\"Longitude: {longitude},\\n\"\n",
    "                f\"City: {data.get('city', 'N/A')},\\n\"\n",
    "                f\"Country: {data.get('country', 'N/A')}\"\n",
    "            )\n",
    "            return data\n",
    "        else:\n",
    "            return \"Location could not be determined.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n",
    "    \n",
    "@tool\n",
    "def get_current_datetime() -> str:\n",
    "    \"\"\"Return the current date and time.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our prompt, this time we'll skip the `chat_history` part as we don't need it. However, you can add it if preferred."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:40:21.901455Z",
     "start_time": "2025-06-12T15:40:21.898372Z"
    }
   },
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you're a helpful assistant\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our full `tools` list, our `agent`, and the `agent_executor`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:40:25.538899Z",
     "start_time": "2025-06-12T15:40:25.532569Z"
    }
   },
   "source": [
    "tools = toolbox + [get_current_datetime, get_location_from_ip]\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=llm, tools=tools, prompt=prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me I have to specify to the AI as I live in the UK and alot of places in the UK also exist in USA, which is why I explicitly state to use the country in the search as well as the town / city."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:42:27.801641Z",
     "start_time": "2025-06-12T15:40:38.918665Z"
    }
   },
   "source": [
    "out = agent_executor.invoke({\n",
    "    \"input\": (\n",
    "        \"I have a few questions, what is the date and time right now? \"\n",
    "        \"How is the weather where I am? Please give me degrees in Celsius\"\n",
    "    )\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m out = \u001B[43magent_executor\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mI have a few questions, what is the date and time right now? \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHow is the weather where I am? Please give me degrees in Celsius\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    166\u001B[39m     run_manager.on_chain_error(e)\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    168\u001B[39m run_manager.on_chain_end(outputs)\n\u001B[32m    170\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    155\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    156\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m157\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    158\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    159\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    160\u001B[39m     )\n\u001B[32m    162\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    163\u001B[39m         inputs, outputs, return_only_outputs\n\u001B[32m    164\u001B[39m     )\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1620\u001B[39m, in \u001B[36mAgentExecutor._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m   1618\u001B[39m \u001B[38;5;66;03m# We now enter the agent loop (until it returns something).\u001B[39;00m\n\u001B[32m   1619\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_continue(iterations, time_elapsed):\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m     next_step_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_take_next_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1621\u001B[39m \u001B[43m        \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1622\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1623\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1624\u001B[39m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1625\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1626\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1627\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(next_step_output, AgentFinish):\n\u001B[32m   1628\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._return(\n\u001B[32m   1629\u001B[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001B[32m   1630\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1328\u001B[39m, in \u001B[36mAgentExecutor._take_next_step\u001B[39m\u001B[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[39m\n\u001B[32m   1317\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_take_next_step\u001B[39m(\n\u001B[32m   1318\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1319\u001B[39m     name_to_tool_map: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, BaseTool],\n\u001B[32m   (...)\u001B[39m\u001B[32m   1323\u001B[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1324\u001B[39m ) -> Union[AgentFinish, \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mtuple\u001B[39m[AgentAction, \u001B[38;5;28mstr\u001B[39m]]]:\n\u001B[32m   1325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._consume_next_step(\n\u001B[32m   1326\u001B[39m         \u001B[43m[\u001B[49m\n\u001B[32m   1327\u001B[39m \u001B[43m            \u001B[49m\u001B[43ma\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m1328\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_iter_next_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1329\u001B[39m \u001B[43m                \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1330\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1331\u001B[39m \u001B[43m                \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1332\u001B[39m \u001B[43m                \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1333\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1334\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1335\u001B[39m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m   1336\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1354\u001B[39m, in \u001B[36mAgentExecutor._iter_next_step\u001B[39m\u001B[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[39m\n\u001B[32m   1351\u001B[39m     intermediate_steps = \u001B[38;5;28mself\u001B[39m._prepare_intermediate_steps(intermediate_steps)\n\u001B[32m   1353\u001B[39m     \u001B[38;5;66;03m# Call the LLM to see what to do.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1354\u001B[39m     output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_action_agent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mplan\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1355\u001B[39m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1356\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1357\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1358\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1359\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m OutputParserException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1360\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.handle_parsing_errors, \u001B[38;5;28mbool\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:577\u001B[39m, in \u001B[36mRunnableMultiActionAgent.plan\u001B[39m\u001B[34m(self, intermediate_steps, callbacks, **kwargs)\u001B[39m\n\u001B[32m    569\u001B[39m final_output: Any = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    570\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream_runnable:\n\u001B[32m    571\u001B[39m     \u001B[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001B[39;00m\n\u001B[32m    572\u001B[39m     \u001B[38;5;66;03m# streaming\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    575\u001B[39m     \u001B[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001B[39;00m\n\u001B[32m    576\u001B[39m     \u001B[38;5;66;03m# accumulate the output into final output and return that.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m577\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrunnable\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    578\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfinal_output\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[32m    579\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfinal_output\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3438\u001B[39m, in \u001B[36mRunnableSequence.stream\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3431\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   3432\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstream\u001B[39m(\n\u001B[32m   3433\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3436\u001B[39m     **kwargs: Optional[Any],\n\u001B[32m   3437\u001B[39m ) -> Iterator[Output]:\n\u001B[32m-> \u001B[39m\u001B[32m3438\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform(\u001B[38;5;28miter\u001B[39m([\u001B[38;5;28minput\u001B[39m]), config, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3424\u001B[39m, in \u001B[36mRunnableSequence.transform\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3417\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   3418\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtransform\u001B[39m(\n\u001B[32m   3419\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3422\u001B[39m     **kwargs: Optional[Any],\n\u001B[32m   3423\u001B[39m ) -> Iterator[Output]:\n\u001B[32m-> \u001B[39m\u001B[32m3424\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._transform_stream_with_config(\n\u001B[32m   3425\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   3426\u001B[39m         \u001B[38;5;28mself\u001B[39m._transform,\n\u001B[32m   3427\u001B[39m         patch_config(config, run_name=(config \u001B[38;5;129;01mor\u001B[39;00m {}).get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.name),\n\u001B[32m   3428\u001B[39m         **kwargs,\n\u001B[32m   3429\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2215\u001B[39m, in \u001B[36mRunnable._transform_stream_with_config\u001B[39m\u001B[34m(self, inputs, transformer, config, run_type, **kwargs)\u001B[39m\n\u001B[32m   2213\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   2214\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2215\u001B[39m         chunk: Output = \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2216\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[32m   2217\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m final_output_supported:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3386\u001B[39m, in \u001B[36mRunnableSequence._transform\u001B[39m\u001B[34m(self, inputs, run_manager, config, **kwargs)\u001B[39m\n\u001B[32m   3383\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3384\u001B[39m         final_pipeline = step.transform(final_pipeline, config)\n\u001B[32m-> \u001B[39m\u001B[32m3386\u001B[39m \u001B[38;5;28;01myield from\u001B[39;00m final_pipeline\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1429\u001B[39m, in \u001B[36mRunnable.transform\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   1426\u001B[39m final: Input\n\u001B[32m   1427\u001B[39m got_first_val = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1429\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43michunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[32m   1430\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# The default implementation of transform is to buffer input and\u001B[39;49;00m\n\u001B[32m   1431\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# then call stream.\u001B[39;49;00m\n\u001B[32m   1432\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# It'll attempt to gather all input into a single chunk using\u001B[39;49;00m\n\u001B[32m   1433\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# the `+` operator.\u001B[39;49;00m\n\u001B[32m   1434\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If the input is not addable, then we'll assume that we can\u001B[39;49;00m\n\u001B[32m   1435\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# only operate on the last chunk,\u001B[39;49;00m\n\u001B[32m   1436\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# and we'll iterate until we get to the last chunk.\u001B[39;49;00m\n\u001B[32m   1437\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mgot_first_val\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1438\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43michunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5647\u001B[39m, in \u001B[36mRunnableBindingBase.transform\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   5640\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   5641\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtransform\u001B[39m(\n\u001B[32m   5642\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   5645\u001B[39m     **kwargs: Any,\n\u001B[32m   5646\u001B[39m ) -> Iterator[Output]:\n\u001B[32m-> \u001B[39m\u001B[32m5647\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.bound.transform(\n\u001B[32m   5648\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   5649\u001B[39m         \u001B[38;5;28mself\u001B[39m._merge_configs(config),\n\u001B[32m   5650\u001B[39m         **{**\u001B[38;5;28mself\u001B[39m.kwargs, **kwargs},\n\u001B[32m   5651\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1447\u001B[39m, in \u001B[36mRunnable.transform\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   1444\u001B[39m             final = ichunk\n\u001B[32m   1446\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m got_first_val:\n\u001B[32m-> \u001B[39m\u001B[32m1447\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream(final, config, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:499\u001B[39m, in \u001B[36mBaseChatModel.stream\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    497\u001B[39m input_messages = _normalize_messages(messages)\n\u001B[32m    498\u001B[39m run_id = \u001B[33m\"\u001B[39m\u001B[33m-\u001B[39m\u001B[33m\"\u001B[39m.join((_LC_ID_PREFIX, \u001B[38;5;28mstr\u001B[39m(run_manager.run_id)))\n\u001B[32m--> \u001B[39m\u001B[32m499\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    500\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mid\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[32m    501\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mid\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_id\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:803\u001B[39m, in \u001B[36mChatOllama._stream\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    796\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_stream\u001B[39m(\n\u001B[32m    797\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    798\u001B[39m     messages: \u001B[38;5;28mlist\u001B[39m[BaseMessage],\n\u001B[32m   (...)\u001B[39m\u001B[32m    801\u001B[39m     **kwargs: Any,\n\u001B[32m    802\u001B[39m ) -> Iterator[ChatGenerationChunk]:\n\u001B[32m--> \u001B[39m\u001B[32m803\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_iterate_over_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    804\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    805\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mon_llm_new_token\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    806\u001B[39m \u001B[43m                \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    807\u001B[39m \u001B[43m                \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    808\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:763\u001B[39m, in \u001B[36mChatOllama._iterate_over_stream\u001B[39m\u001B[34m(self, messages, stop, **kwargs)\u001B[39m\n\u001B[32m    756\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_iterate_over_stream\u001B[39m(\n\u001B[32m    757\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    758\u001B[39m     messages: \u001B[38;5;28mlist\u001B[39m[BaseMessage],\n\u001B[32m    759\u001B[39m     stop: Optional[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    760\u001B[39m     **kwargs: Any,\n\u001B[32m    761\u001B[39m ) -> Iterator[ChatGenerationChunk]:\n\u001B[32m    762\u001B[39m     is_thinking = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m763\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    764\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    765\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdone\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:665\u001B[39m, in \u001B[36mChatOllama._create_chat_stream\u001B[39m\u001B[34m(self, messages, stop, **kwargs)\u001B[39m\n\u001B[32m    662\u001B[39m chat_params = \u001B[38;5;28mself\u001B[39m._chat_params(messages, stop, **kwargs)\n\u001B[32m    664\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chat_params[\u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m665\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client.chat(**chat_params)\n\u001B[32m    666\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    667\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client.chat(**chat_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\ollama\\_client.py:172\u001B[39m, in \u001B[36mClient._request.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    169\u001B[39m   e.response.read()\n\u001B[32m    170\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e.response.text, e.response.status_code) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m.\u001B[49m\u001B[43miter_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m  \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[43m.\u001B[49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m  \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43merr\u001B[49m\u001B[43m \u001B[49m\u001B[43m:=\u001B[49m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43merror\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpx\\_models.py:929\u001B[39m, in \u001B[36mResponse.iter_lines\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    927\u001B[39m decoder = LineDecoder()\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m929\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpx\\_models.py:916\u001B[39m, in \u001B[36mResponse.iter_text\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m    914\u001B[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001B[32m    915\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m916\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbyte_content\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtext_content\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbyte_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    918\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_content\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpx\\_models.py:897\u001B[39m, in \u001B[36mResponse.iter_bytes\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m    895\u001B[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001B[32m    896\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m897\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraw_bytes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    898\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecoded\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    899\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoded\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpx\\_models.py:951\u001B[39m, in \u001B[36mResponse.iter_raw\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m    948\u001B[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001B[32m    950\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m951\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_num_bytes_downloaded\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpx\\_client.py:153\u001B[39m, in \u001B[36mBoundSyncStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    154\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:127\u001B[39m, in \u001B[36mResponseStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    125\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m    126\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_httpcore_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:407\u001B[39m, in \u001B[36mPoolByteStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    405\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    406\u001B[39m     \u001B[38;5;28mself\u001B[39m.close()\n\u001B[32m--> \u001B[39m\u001B[32m407\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:403\u001B[39m, in \u001B[36mPoolByteStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    401\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:342\u001B[39m, in \u001B[36mHTTP11ConnectionByteStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ShieldCancellation():\n\u001B[32m    341\u001B[39m     \u001B[38;5;28mself\u001B[39m.close()\n\u001B[32m--> \u001B[39m\u001B[32m342\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:334\u001B[39m, in \u001B[36mHTTP11ConnectionByteStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    332\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    333\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mreceive_response_body\u001B[39m\u001B[33m\"\u001B[39m, logger, \u001B[38;5;28mself\u001B[39m._request, kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m334\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_receive_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    335\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    337\u001B[39m     \u001B[38;5;66;03m# If we get an exception while streaming the response,\u001B[39;00m\n\u001B[32m    338\u001B[39m     \u001B[38;5;66;03m# we want to close the response (and possibly the connection)\u001B[39;00m\n\u001B[32m    339\u001B[39m     \u001B[38;5;66;03m# before raising that exception.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:203\u001B[39m, in \u001B[36mHTTP11Connection._receive_response_body\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    200\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m     event = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    204\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Data):\n\u001B[32m    205\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mbytes\u001B[39m(event.data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001B[39m, in \u001B[36mHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_network_stream\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\engineering\\ml\\langchain\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001B[39m, in \u001B[36mSyncStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    127\u001B[39m     \u001B[38;5;28mself\u001B[39m._sock.settimeout(timeout)\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out[\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the correct answer, and we even get the approximate answer in Celsius despite the tool returning the temperature in Fahrenheit.\n",
    "\n",
    "We've finished our into to LangChain Agents, in the next chapter we will be looking at how to create custom agents and executors.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
